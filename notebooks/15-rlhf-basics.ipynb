{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 - RLHF Basics with trl (HuggingFace)\n",
    "# pip install trl datasets transformers accelerate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load a small model for demo (you can also use 'gpt2')\n",
    "model_name = \"lvwerra/gpt2-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=None,\n",
    "    batch_size=2,\n",
    "    mini_batch_size=1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, model, tokenizer)\n",
    "\n",
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:10]\")  # keep small for demo\n",
    "\n",
    "for example in dataset:\n",
    "    prompt = example[\"text\"][:100]  # limit input length\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=20)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Reward function (very naive): reward longer outputs\n",
    "    reward = torch.tensor([float(len(decoded_output)) / 100.0])\n",
    "\n",
    "    # Run PPO step\n",
    "    ppo_trainer.step([prompt], [decoded_output], reward)\n",
    "    print(f\"Trained on: {prompt[:30]}... => Reward: {reward.item():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
