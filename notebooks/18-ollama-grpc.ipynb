{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from concurrent import futures\n",
    "import ollama_pb2\n",
    "import ollama_pb2_grpc\n",
    "\n",
    "class LLMServiceServicer(ollama_pb2_grpc.LLMServiceServicer):\n",
    "    def Prompt(self, request, context):\n",
    "        prompt = request.input\n",
    "        # Here you'd call your Ollama LLM logic\n",
    "        result = f\"Echo: {prompt}\"  # Replace with real LLM output\n",
    "        return ollama_pb2.PromptResponse(output=result)\n",
    "\n",
    "def serve():\n",
    "    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n",
    "    ollama_pb2_grpc.add_LLMServiceServicer_to_server(LLMServiceServicer(), server)\n",
    "    server.add_insecure_port('[::]:50051')\n",
    "    server.start()\n",
    "    print(\"gRPC server running at port 50051...\")\n",
    "    server.wait_for_termination()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    serve()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
